{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "foreign-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import xgboost as xgb \n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "processed-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load pytorch tensor data from file\n",
    "\n",
    "    Args:\n",
    "        filepath (str): a full filename path.\n",
    "\n",
    "    Returns:\n",
    "        X (array): a numpy array of features.\n",
    "        y (array): a numpy array of labels.\n",
    "    \"\"\"\n",
    "    tensors = torch.load(file_path)\n",
    "    X = tensors[:, :-1].copy()\n",
    "    y = tensors[:, -1].copy().astype(int)\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "great-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_input_path = os.path.join('..', 'data', 'processed')\n",
    "X_train, y_train   = load_data(os.path.join(tensors_input_path, 'mozilla_bug_report_train_data.pt'))\n",
    "X_test, y_test   = load_data(os.path.join(tensors_input_path, 'mozilla_bug_report_test_data.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "african-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    print(\"Training with params : \")\n",
    "    print(params)\n",
    "    \n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "   \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    model  = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    predictions = model.predict(dvalid).reshape((X_test.shape[0], 5))\n",
    "    \n",
    "    score = log_loss(y_test, predictions)\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'num_class' : 5,\n",
    "             'eval_metric': 'mlogloss',\n",
    "             'objective': 'multi:softprob',\n",
    "             'nthread' : 6,\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=10)\n",
    "\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "solid-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params :                                \n",
      "{'colsample_bytree': 0.9500000000000001, 'eta': 0.17500000000000002, 'eval_metric': 'mlogloss', 'gamma': 0.55, 'max_depth': 13, 'min_child_weight': 2.0, 'n_estimators': 477.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.55}\n",
      "[00:06:50] WARNING: ../src/learner.cc:541:            \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.9344406914710999                             \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 1.0, 'eta': 0.4, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'max_depth': 11, 'min_child_weight': 6.0, 'n_estimators': 633.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.8}\n",
      "[00:07:11] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 2.0469632506370545                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.75, 'eta': 0.325, 'eval_metric': 'mlogloss', 'gamma': 0.8500000000000001, 'max_depth': 6, 'min_child_weight': 3.0, 'n_estimators': 925.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.9500000000000001}\n",
      "[00:07:28] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.8367634207010268                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.8, 'eta': 0.05, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'max_depth': 7, 'min_child_weight': 5.0, 'n_estimators': 713.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.5}\n",
      "[00:09:15] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.8594675767421722                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.8, 'eta': 0.375, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'max_depth': 7, 'min_child_weight': 2.0, 'n_estimators': 638.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.5}\n",
      "[00:10:01] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 2.1936973217129707                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.55, 'eta': 0.05, 'eval_metric': 'mlogloss', 'gamma': 0.9500000000000001, 'max_depth': 5, 'min_child_weight': 3.0, 'n_estimators': 321.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.5}\n",
      "[00:10:39] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.8291342902183532                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.6000000000000001, 'eta': 0.35000000000000003, 'eval_metric': 'mlogloss', 'gamma': 0.75, 'max_depth': 4, 'min_child_weight': 3.0, 'n_estimators': 151.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.9}\n",
      "[00:10:52] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.8139175379276276                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.8500000000000001, 'eta': 0.45, 'eval_metric': 'mlogloss', 'gamma': 0.55, 'max_depth': 12, 'min_child_weight': 4.0, 'n_estimators': 927.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.6000000000000001}\n",
      "[00:11:05] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 2.0861442086100577                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.9, 'eta': 0.07500000000000001, 'eval_metric': 'mlogloss', 'gamma': 0.65, 'max_depth': 7, 'min_child_weight': 6.0, 'n_estimators': 911.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.8500000000000001}\n",
      "[00:11:38] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 1.9918304979801178                                                       \n",
      "\n",
      "\n",
      "Training with params :                                                          \n",
      "{'colsample_bytree': 0.6000000000000001, 'eta': 0.375, 'eval_metric': 'mlogloss', 'gamma': 0.55, 'max_depth': 12, 'min_child_weight': 4.0, 'n_estimators': 253.0, 'nthread': 6, 'num_class': 5, 'objective': 'multi:softprob', 'silent': 1, 'subsample': 0.6000000000000001}\n",
      "[00:12:22] WARNING: ../src/learner.cc:541:                                      \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 2.0351434448361396                                                       \n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [05:38<00:00, 33.90s/trial, best loss: 1.8139175379276276]\n",
      "{'colsample_bytree': 0.6000000000000001, 'eta': 0.35000000000000003, 'gamma': 0.75, 'max_depth': 3, 'min_child_weight': 3.0, 'n_estimators': 151.0, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "domestic-israel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:36:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "model = xgb.train({'colsample_bytree': 0.6000000000000001, \n",
    "                   'eta': 0.35000000000000003, 'gamma': 0.75, 'max_depth': 3, \n",
    "                   'min_child_weight': 3.0, 'subsample': 0.9},\n",
    "                   dtrain, 151,\n",
    "                   objective= 'multi:softmax',\n",
    "                   num_class= 5, )\n",
    "y_pred = model.predict(dvalid).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aquatic-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3, 2, 3, 3, 2, 1, 2, 4, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 3,\n",
       "       2, 2, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "shared-fishing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 3, 2, 1, 0, 3, 3, 1, 2, 3, 2, 1, 1, 1, 0, 2, 3, 2, 4, 4,\n",
       "       3, 2, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "internal-chapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.43      0.50      0.46         6\n",
      "           2       0.30      0.38      0.33         8\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.28        25\n",
      "   macro avg       0.19      0.20      0.19        25\n",
      "weighted avg       0.25      0.28      0.26        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-opening",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
